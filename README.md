# NSMN-Tupper: Active Vision & Holographic Codebooks

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

**NSMN-Tupper** (Neural Saccadic Memory Network) es una arquitectura de visi√≥n computacional ultra-eficiente que combina **mecanismos de sacadas biol√≥gicas** con la **F√≥rmula Autoreferencial de Tupper** para lograr un rendimiento competitivo con una fracci√≥n de los par√°metros de las redes convencionales.

## üöÄ Resumen del Desempe√±o
| Modelo | Par√°metros | √âpocas | CIFAR-10 (Acc@1) |
| :--- | :--- | :--- | :--- |
| ResNet-18 | 11.2M | 200 | ~93.0% |
| **NSMN-Tupper** | **3.5M** | **120 (93% @ 65ep)** | **93.2%** |

## üß† ¬øQu√© hace √∫nico a este modelo?

A diferencia de las CNN convencionales que procesan la imagen de forma est√°tica, NSMN-Tupper utiliza tres pilares innovadores:

1.  **Visi√≥n Activa (Saccadic Glimpses):** El modelo no "mira" toda la imagen a la vez. Utiliza una red de localizaci√≥n que selecciona secuencialmente las zonas m√°s informativas (glimpses) a diferentes escalas, emulando el movimiento ocular humano.
2.  **Memoria Recurrente (GRU):** La informaci√≥n extra√≠da de cada "sacada" se integra en una unidad de memoria recurrente (GRU), permitiendo que el modelo refine su predicci√≥n con cada vistazo.
3.  **Codebook Hologr√°fico de Tupper:** En lugar de clasificar solo mediante un vector *one-hot*, el modelo debe "reconstruir" un patr√≥n binario derivado de la **F√≥rmula Autoreferencial de Tupper**. Esto act√∫a como un regularizador de alta entrop√≠a que previene el sobreajuste y obliga a la red a aprender caracter√≠sticas estructurales profundas.

## üèóÔ∏è Arquitectura

El flujo de datos se divide en:
- **ContextNet:** Una ResNet ligera que genera el mapa de caracter√≠sticas inicial.
- **LocationNetwork:** Predice las coordenadas $(x, y)$ del siguiente vistazo.
- **GlimpseNet:** Extrae parches multiescala mediante transformaciones afines.
- **Holographic Head:** Proyecta la memoria final hacia el espacio de bits de Tupper para la clasificaci√≥n por similitud de coseno.

```python
# La magia de la eficiencia
loss = loss_cross_entropy + 2.0 * loss_tupper_reconstruction
```

## üõ†Ô∏è Instalaci√≥n y Uso

### Requisitos
- Python 3.8+
- PyTorch (CUDA compatible recomendado)
- Torchvision
- Matplotlib

### Ejecuci√≥n
Para entrenar el modelo desde cero:
```bash
python main.py
```

## üìä Visualizaci√≥n de Resultados

El modelo no solo predice la clase, sino que "imagina" el c√≥digo de Tupper asociado:

| Imagen Original | Tupper Generado (Predicci√≥n) | Objetivo (Codebook) |
| :---: | :---: | :---: |
| ![Car](https://via.placeholder.com/100?text=Car) | ![Gen](https://via.placeholder.com/100?text=Bits) | ![Target](https://via.placeholder.com/100?text=Tupper) |

*Nota: Durante el entrenamiento, el modelo aprende a mapear las caracter√≠sticas visuales hacia la constante de Tupper de forma determinista.*

## üìà Curva de Aprendizaje
Gracias al optimizador `OneCycleLR` y al codebook fijo, el modelo presenta una convergencia extremadamente r√°pida, alcanzando el **90% de precisi√≥n en menos de 40 √©pocas**.

## üìÑ Licencia
Este proyecto est√° bajo la Licencia MIT.

## ü§ù Cr√©ditos
Desarrollado como una exploraci√≥n en arquitecturas de visi√≥n eficientes y regularizaci√≥n geom√©trica. Inspirado en el trabajo de Kaiming He (ResNet) y Jeff Tupper (Tupper's Self-Referential Formula).
